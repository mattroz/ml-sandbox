{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from planar_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using helper function and plot it\n",
    "N = 400\n",
    "X, Y = sklearn.datasets.make_moons(n_samples=N, noise=.15)\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(X, Y, hidden_layer_size, learning_rate=0.05, function='sigmoid'):\n",
    "    \"\"\"\n",
    "    Weights and biases initialization\n",
    "    \n",
    "    Arguments: \n",
    "        X - input data\n",
    "        Y - labels\n",
    "        hidden_layer_size - number of neurons in the hidden layer\n",
    "        activation_function - name of preferred activation\n",
    "    Returns:\n",
    "        parameters - dictionary containing weights and biases\n",
    "    \"\"\"\n",
    "    n_inputs = X.shape[0]\n",
    "    n_outputs = Y.shape[0]\n",
    "    squash_coef = 0.01\n",
    "    \n",
    "    # The first layer\n",
    "    W1 = np.random.randn(hidden_layer_size, n_inputs) * squash_coef\n",
    "    b1 = np.zeros((hidden_layer_size, 1))\n",
    "    # The second layer\n",
    "    W2 = np.random.randn(n_outputs, hidden_layer_size) * squash_coef\n",
    "    b2 = np.zeros((n_outputs, 1))\n",
    "    \n",
    "    activation = {\n",
    "        'sigmoid': sigmoid,\n",
    "        'relu': ReLU,\n",
    "        'tanh': np.tanh\n",
    "    }\n",
    "    \n",
    "    activation_derivative = {\n",
    "        'sigmoid': sigmoid_derivative,\n",
    "        'tanh': tanh_derivative,\n",
    "        'relu': ReLU_derivative\n",
    "    }\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'learning_rate': learning_rate,\n",
    "        'activation': activation[function],\n",
    "        'activation_derivative': activation_derivative[function]\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Apply forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "        X - input_data\n",
    "        parameters - dictionary from initialize() function\n",
    "    Returns:\n",
    "        hidden_parameters - dictionary containing dot products and activations of dot products for layers\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = parameters['activation'](Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    hidden_parameters = {\n",
    "        'Z1': Z1,\n",
    "        'A1': A1,\n",
    "        'Z2': Z2,\n",
    "        'A2': A2    \n",
    "    }\n",
    "    \n",
    "    return hidden_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss\n",
    "    \n",
    "    Arguments:\n",
    "        Y_hat - predicted labels\n",
    "        Y - true labels\n",
    "    Returns:\n",
    "        Scalar cross-entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    logp = np.multiply(np.log(Y_hat), Y) + np.multiply(np.log(1 - Y_hat), (1 - Y))\n",
    "    _cost = -(1/m) * np.sum(logp)\n",
    "    _cost = np.squeeze(_cost)\n",
    "    \n",
    "    assert(isinstance(_cost, float))\n",
    "    \n",
    "    return _cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, parameters, hidden_parameters):\n",
    "    \"\"\"\n",
    "    Apply back propagation using computed parameters\n",
    "    \n",
    "    Arguments:\n",
    "        X - input data\n",
    "        Y - true labels\n",
    "        parameters - weights, biases, functions, etc\n",
    "        hidden_parameters - dot products and activations\n",
    "    Returns:\n",
    "        parameters - updated parameters after backprop step\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Get weights and biases\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    act = parameters['activation']\n",
    "    l_r = parameters['learning_rate']\n",
    "    act_derivative = parameters['activation_derivative']\n",
    "\n",
    "    # Get dot products and activations\n",
    "    Z1 = hidden_parameters['Z1']\n",
    "    A1 = hidden_parameters['A1']\n",
    "    Z2 = hidden_parameters['Z2']\n",
    "    A2 = hidden_parameters['A2']\n",
    "    \n",
    "    # Compute derivatives\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(W2.T*dZ2, act_derivative(A1))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= l_r * dW1\n",
    "    b1 -= l_r * db1\n",
    "    W2 -= l_r * dW2\n",
    "    b2 -= l_r * db2\n",
    "    \n",
    "    # Rewrite parameters\n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2,\n",
    "        'learning_rate': l_r,\n",
    "        'activation': act,\n",
    "        'activation_derivative': act_derivative\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    output = forward_propagation(X, parameters)\n",
    "    return np.where(output['A2'] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(X, Y, n_hidd_units, n_iters, learning_rate=0.01, function='sigmoid'):\n",
    "    parameters = initialize(X, Y, n_hidd_units, learning_rate, function)\n",
    "    for i in range(n_iters):\n",
    "        hidden_parameters = forward_propagation(X, parameters)\n",
    "        _cost = cost(hidden_parameters['A2'], Y)\n",
    "        parameters = back_propagation(X, Y, parameters, hidden_parameters)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print('Cost on %d iteration: %f' % (i, _cost))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidd_units = 5\n",
    "trained_nn = NN_model(X, Y, hidd_units, 10000, function='tanh')\n",
    "plot_decision_boundary(lambda x: predict(x.T, trained_nn), X, Y)\n",
    "plt.title(\"Decision boundary for number of hidden units: \" + str(hidd_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "predictions = predict(X, trained_nn)\n",
    "print('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1-Y, 1-predictions.T))/float(Y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
