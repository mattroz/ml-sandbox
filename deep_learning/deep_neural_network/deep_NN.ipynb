{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from testCases_v2 import *\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_size):\n",
    "    \"\"\"\n",
    "    Returns parameters ('W1', 'b1', W2', 'b2', ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    for l in range(1, len(layers_size)):\n",
    "        squash_coeff = 1 / np.sqrt(layers_size[l-1])\n",
    "        print(squash_coeff)\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_size[l], layers_size[l-1]) * squash_coeff\n",
    "        parameters['b' + str(l)] = np.zeros((layers_size[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearize_and_activate_forward_unit(A_previous, W, b, activation_function='relu'):\n",
    "    \"\"\"\n",
    "    Returns A, layer_parameters(W, b, Z, A)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation_function == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    \n",
    "    layer_parameters = {\n",
    "        'W': W,\n",
    "        'b': b,\n",
    "        'Z': Z,\n",
    "        'A': A_previous\n",
    "    }\n",
    "    \n",
    "    return A, layer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_forward(X, parameters):\n",
    "    n_of_layers = len(parameters) // 2\n",
    "    A = X\n",
    "    layers_parameters = []\n",
    "    \n",
    "    W = parameters\n",
    "    \n",
    "    for l in range(1, n_of_layers):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, params = linearize_and_activate_forward_unit(A, W, b, 'relu')\n",
    "        layers_parameters.append(params)\n",
    "    \n",
    "    # Last layer\n",
    "    W = parameters['W' + str(n_of_layers)]\n",
    "    b = parameters['b' + str(n_of_layers)]\n",
    "    A_last, params = linearize_and_activate_forward_unit(A, W, b, 'sigmoid')\n",
    "    layers_parameters.append(params)\n",
    "    \n",
    "    return A_last, layers_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    log_probs = np.multiply(Y, np.log(Y_hat)) + np.multiply(1 - Y, np.log(1 - Y_hat))\n",
    "    cost = -(1/m) * np.sum(log_probs)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_back_activation(dA, Z, activation):\n",
    "    \"\"\"\n",
    "    Returns dZ[L]\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add A_prev, W, b to the cache\n",
    "def propagate_back_linear(layer_parameters, dZ):\n",
    "    \"\"\"\n",
    "    Returns dA, dW, db\n",
    "    \"\"\"\n",
    "    W = layer_parameters['W']\n",
    "    b = layer_parameters['b']\n",
    "    A_previous = layer_parameters['A']\n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A_previous.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_previous = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_unit(dA, layer_parameters, activation):\n",
    "    W = layer_parameters['W']\n",
    "    b = layer_parameters['b']\n",
    "    Z = layer_parameters['Z']\n",
    "    A_previous = layer_parameters['A']\n",
    "    dZ = propagate_back_activation(dA, Z, activation)\n",
    "    dA_previous, dW, db = propagate_back_linear(layer_parameters, dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_backward(AL, Y, layers_parameters):\n",
    "    \"\"\"\n",
    "    layer_parameters: W, b, Z, A\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    layers_number = len(layers_parameters)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    gradients = {}\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    gradients['dA' + str(layers_number)], \\\n",
    "    gradients['dW' + str(layers_number)], \\\n",
    "    gradients['db' + str(layers_number)] = backward_propagation_unit(dAL, layers_parameters[-1], 'sigmoid')\n",
    "    \n",
    "    for l in reversed(range(layers_number-1)):\n",
    "        current_layer_params = layers_parameters[l]\n",
    "        gradients['dA' + str(l+1)], \\\n",
    "        gradients['dW' + str(l+1)], \\\n",
    "        gradients['db' + str(l+1)] = backward_propagation_unit(gradients['dA' + str(l+2)], \n",
    "                                                               current_layer_params, \n",
    "                                                               'relu') \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(parameters, gradients, learning_rate):\n",
    "    n_of_layers = len(parameters) // 2\n",
    "    \n",
    "    for l in range(n_of_layers):\n",
    "        parameters['W' + str(l+1)] -= learning_rate * gradients['dW' + str(l+1)]\n",
    "        parameters['b' + str(l+1)] -= learning_rate * gradients['db' + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, layers_size):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    # Forward propagation\n",
    "    probas, params = deep_forward(X, parameters)\n",
    "\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_NN_model(X, Y, layers_size, learning_rate=0.0075, n_iters=2500):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    costs = []\n",
    "    parameters = initialize_parameters(layers_size)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        A_last, layers_parameters = deep_forward(X, parameters)\n",
    "        cost = calculate_cost(A_last, Y)\n",
    "        gradients = deep_backward(A_last, Y, layers_parameters)\n",
    "        parameters = update_weights(parameters, gradients, learning_rate)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            #print(gradients)\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00902109795609\n",
      "0.22360679775\n",
      "0.377964473009\n",
      "0.4472135955\n",
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672665\n",
      "Cost after iteration 200: 0.649577\n",
      "Cost after iteration 300: 0.610020\n",
      "Cost after iteration 400: 0.554252\n",
      "Cost after iteration 500: 0.507791\n",
      "Cost after iteration 600: 0.448387\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-ce427e73abab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m12288\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_NN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-40cb4d2c5400>\u001b[0m in \u001b[0;36mdeep_NN_model\u001b[0;34m(X, Y, layers_size, learning_rate, n_iters)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-4f0fc8c63641>\u001b[0m in \u001b[0;36mdeep_backward\u001b[0;34m(AL, Y, layers_parameters)\u001b[0m\n\u001b[1;32m     16\u001b[0m         gradients['dA' + str(l+1)],         gradients['dW' + str(l+1)],         gradients['db' + str(l+1)] = backward_propagation_unit(gradients['dA' + str(l+2)], \n\u001b[1;32m     17\u001b[0m                                                                \u001b[0mcurrent_layer_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                                'relu') \n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-c6afca919ee8>\u001b[0m in \u001b[0;36mbackward_propagation_unit\u001b[0;34m(dA, layer_parameters, activation)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mA_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#layer_parameters[0][0]\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate_back_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate_back_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-3badd75e3f0f>\u001b[0m in \u001b[0;36mpropagate_back_linear\u001b[0;34m(layer_parameters, dZ)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_previous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdA_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdA_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers_size = [12288, 20, 7, 5, 1]\n",
    "parameters = deep_NN_model(train_x, train_y, layers_size, learning_rate=0.0075, n_iters=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655502392344\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters, layers_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters, layers_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
