{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_size):\n",
    "    \"\"\"\n",
    "    Returns parameters ('W1', 'b1', W2', 'b2', ...)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    for l in range(1, len(layers_size)):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_size[l], layers_size[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers_size[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_and_activate_forward_unit(A_previous, W, b, activation_function='relu'):\n",
    "    \"\"\"\n",
    "    Returns A, layer_parameters(W, b, Z, A)\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A_previous) + b\n",
    "    if activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    \n",
    "    layer_parameters = {\n",
    "        'W': W,\n",
    "        'b': b,\n",
    "        'Z': Z,\n",
    "        'A': A_previous\n",
    "    }\n",
    "    \n",
    "    return A, layer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_forward(X, parameters):\n",
    "    n_of_layers = parameters // 2\n",
    "    A = X\n",
    "    layers_parameters = []\n",
    "    \n",
    "    W = parameters\n",
    "    \n",
    "    for l in range(1, n_of_layers):\n",
    "        W = parameters['W' + str(l)]\n",
    "        b = parameters['b' + str(l)]\n",
    "        A, params = linearize_and_activate_forward(A, w, b, 'relu')\n",
    "        layers_parameters.append(params)\n",
    "    # Last layer\n",
    "    W = parameters['W' + str(n_of_layers-1)]\n",
    "    b = parameters['b' + str(n_of_layers-1)]\n",
    "    A_last, params = linearize_and_activate_forward(A, W, b, 'sigmoid')\n",
    "    layers_parameters.append(params)\n",
    "    \n",
    "    return A_last, layers_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(Y_hat, Y):\n",
    "    \"\"\"\n",
    "    Returns cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    log_probs = np.dot(Y, np.log(Y_hat)) + np.dot(1 - Y, np.log(1 - Y_hat))\n",
    "    cost = -(1/m) * np.sum(log_probs)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_back_activation(dA, Z, activation):\n",
    "    \"\"\"\n",
    "    Returns dZ[L]\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add A_prev, W, b to the cache\n",
    "def propagate_back_linear(layer_parameters, dZ):\n",
    "    \"\"\"\n",
    "    Returns dA, dW, db\n",
    "    \"\"\"\n",
    "    W = layer_parameters['W']\n",
    "    b = layer_parameters['b']\n",
    "    A_previous = layer_parameters['A']\n",
    "    m = A_previous.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A_previous.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_previous = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation_unit(dA, layer_parameters, activation):\n",
    "    W = layer_parameters['W']\n",
    "    b = layer_parameters['b']\n",
    "    Z = layer_parameters['Z']\n",
    "    A_previous = layer_parameters['A']\n",
    "    dZ = propagate_back_activation(dA, Z, activation)\n",
    "    dA_previous, dW, db = propagate_back_linear(A_previous, W, b, dZ)\n",
    "    \n",
    "    return dA_previous, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_backward(AL, Y, layer_parameters):\n",
    "    \"\"\"\n",
    "    layer_parameters: W, b, Z, A\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    layers_number = len(layer_parameters)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    gradients = {}\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    gradients['dA' + str(layers_number-1)], \\\n",
    "    gradients['dW' + str(layers_number)], \\\n",
    "    gradients['dW' + str(layers_number)] = backward_propagation_unit(dAL, layer_parameters, 'sigmoid')\n",
    "    \n",
    "    for l in range(layers_number-1, 0):\n",
    "        current_layer_params = layer_parameters[l]\n",
    "        gradients['dA' + str(l)], \\\n",
    "        gradients['dW' + str(l+1)], \\\n",
    "        gradients['db' + str(l+1)] = backward_propagation_unit(gradients['A' + str(l+2)], \n",
    "                                                               layer_parameters, \n",
    "                                                               'relu') \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(parameters, gradients, learning_rate):\n",
    "    n_of_layers = len(parameters) // 2\n",
    "    \n",
    "    for l in range(n_of_layers):\n",
    "        parameters['W' + str(l+1)] = gradients[''] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
